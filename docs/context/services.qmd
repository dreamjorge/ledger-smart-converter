---
title: "Services Layer Context"
format: html
---

## Purpose

Business logic layer that orchestrates imports, rules, analytics, and data access.

## Architecture

**Location**: `src/services/`

**Pattern**: Service layer separates business logic from UI and domain

## Key Services

### Import Service

**File**: `src/services/import_service.py`

**Key Functions**:

```python
resolve_output_paths(
    data_dir: Path,
    bank_label: str,
    bank_id: str,
    analytics_targets: Dict[str, Tuple[str, str]],
) -> Tuple[Path, Path, Path]
# Returns (out_csv, out_unknown, out_suggestions)

copy_csv_to_analysis(
    data_dir: Path,
    analytics_targets: Dict[str, Tuple[str, str]],
    bank_label: str,
    csv_path: Path,
    bank_id: Optional[str] = None,
) -> Tuple[bool, str]
# Copies CSV into analysis hierarchy

get_csv_last_updated(path: Path) -> Optional[str]
# Returns formatted last modified timestamp or None
```

**Responsibilities**:
- Map bank IDs to output paths
- Atomic CSV file operations
- Track file timestamps

**Output Path Convention**:
- Canonical hierarchy is `data/<bank_dir>/...`
- If no explicit UI mapping exists, `resolve_output_paths()` now falls back to `data/<bank_id>/firefly_<bank_id>.csv`
- `copy_csv_to_analysis()` supports the same `bank_id` fallback when display labels do not match configured map keys

### Rule Service

**File**: `src/services/rule_service.py`

**Key Functions**:

```python
get_pending_count(pending_path: Path) -> int
# Count rules in pending file

stage_rule_change(
    rules_path: Path,
    pending_path: Path,
    merchant_name: str,
    regex_pattern: str,
    expense_account: str,
    bucket_tag: str,
    db_path: Optional[Path] = None
) -> Tuple[bool, Dict]
# Stage a new rule, detect conflicts

merge_pending_rules(
    rules_path: Path,
    pending_path: Path,
    backup_dir: Path,
    db_path: Optional[Path] = None
) -> Tuple[bool, Dict]
# Merge pending to main, create backup

record_recategorization_event(
    db_path: Path,
    transaction_source_hash: str,
    old_category: str,
    new_category: str,
    reason: str = ""
) -> int
# Persist recategorization audit event
```

**Safe Workflow**:
1. User corrects category in UI
2. Rule staged in `rules.pending.yml`
3. Conflict detection runs
4. If clean, merge with backup
5. ML model retrains
6. Pending file cleared

**Conflict Detection**:
- Checks if regex already exists
- Prevents duplicate merchant rules
- Returns conflict details for resolution

**Audit Trail**:
- `stage_rule_change(..., db_path=...)` logs `rule_staged` in DB `audit_events`
- `merge_pending_rules(..., db_path=...)` logs `rules_merged`
- `record_recategorization_event(...)` logs `recategorization` with payload metadata

### Analytics Service

**File**: `src/services/analytics_service.py`

**Key Functions**:

```python
is_categorized(destination_name: str) -> bool
# Check if transaction is properly categorized

calculate_categorization_stats(
    df: pd.DataFrame,
    period: Optional[str] = None,
    start_date: Optional[datetime] = None,
    end_date: Optional[datetime] = None
) -> Dict
# Calculate metrics for dashboard

calculate_categorization_stats_from_db(
    db_path: Path,
    bank_id: Optional[str] = None,
    period: Optional[str] = None,
    start_date: Optional[datetime] = None,
    end_date: Optional[datetime] = None
) -> Dict
# SQL-backed stats from `transactions` table
```

**Stats Returned**:
```python
{
    "total": int,                # Total transactions
    "categorized": int,          # Properly categorized count
    "uncategorized": int,        # Missing category count
    "coverage_pct": float,       # Categorization percentage
    "category_pct": float,       # Category field populated %
    "category_populated": int,   # Has category_name
    "total_spent": float,        # Sum of withdrawal amounts
    "type_counts": Dict,         # Counts by transaction type
    "categories": Dict,          # Counts by category
    "category_spending": Dict,   # Spending by category
    "monthly_spending_trends": Dict  # Monthly aggregates
}
```

**Filtering**:
- Period filtering: `period:YYYY-MM` tag
- Date range: start_date and end_date parameters
- Only withdrawal transactions for spending

### Data Service

**File**: `src/services/data_service.py`

**Key Functions**:

```python
get_csv_path(bank_id: str) -> Optional[Path]
# Map bank ID to CSV file path

load_transactions_from_csv(bank_id: str) -> pd.DataFrame
# Load and parse CSV with date conversion

load_transactions_from_db(bank_id: str, db_path: Optional[Path] = None) -> pd.DataFrame
# Load transactions from SQLite `transactions` table

load_transactions(bank_id: str, prefer_db: bool = True, db_path: Optional[Path] = None) -> pd.DataFrame
# DB-first loader with CSV fallback

load_all_bank_data() -> Dict[str, pd.DataFrame]
# Load all banks at once
```

**Bank Mapping**:
```python
BANK_FILE_MAP = {
    "santander": Path("data/santander/firefly_likeu.csv"),
    "santander_likeu": Path("data/santander/firefly_likeu.csv"),
    "hsbc": Path("data/hsbc/firefly_hsbc.csv"),
}
```

**Error Handling**:
- Returns empty DataFrame on missing file
- Logs errors but doesn't crash
- Validates date column parsing

**Default Runtime Source**:
- UI analytics now uses `load_transactions(..., prefer_db=True)` and `data/ledger.db`.
- When DB is missing or has no rows for a bank, service falls back to CSV.

### Database Service

**File**: `src/services/db_service.py`

**Key Functions**:

```python
class DatabaseService:
    def initialize(self) -> None
    # Creates/updates SQLite schema from src/database/schema.sql

    def upsert_account(
        self,
        account_id: str,
        display_name: str,
        account_type: str = "credit_card",
        bank_id: Optional[str] = None,
        closing_day: Optional[int] = None,
        currency: str = "MXN",
    ) -> None
    # Creates or updates canonical account metadata

    def record_import(
        self,
        bank_id: str,
        source_file: str,
        status: str,
        row_count: int = 0,
        error: Optional[str] = None,
    ) -> int
    # Inserts import audit entry and returns import_id

    def insert_transaction(self, txn: Dict[str, Any], import_id: Optional[int] = None) -> bool
    # Inserts transaction row with source_hash deduplication

    def backfill_normalized_descriptions(self, normalizer) -> int
    # Fills missing normalized_description and raw_description in existing rows
```

**Schema File**: `src/database/schema.sql` (`accounts`, `imports`, `rules`, `transactions`)
**Transaction Text Fields**:
- `description`: legacy compatibility text
- `raw_description`: original parsed text
- `normalized_description`: deterministic normalized text for categorization

### CSV → DB Migration

**Files**:
- `src/csv_to_db_migrator.py` (core migration logic)
- `scripts/migrate_csv_to_db.py` (CLI wrapper)

**CLI**:
```bash
python scripts/migrate_csv_to_db.py \
  --db data/ledger.db \
  --data-dir data \
  --accounts config/accounts.yml
```

**Behavior**:
- Discovers `data/**/firefly*.csv`
- Inserts/updates canonical accounts
- Records import runs in `imports`
- Migrates transactions with `source_hash` deduplication
- Writes both `raw_description` and `normalized_description`
- Performs backfill for rows still missing `normalized_description`

### DB Pipeline (One Command)

**Files**:
- `src/db_pipeline.py` (pipeline orchestration)
- `scripts/run_db_pipeline.py` (CLI wrapper)

**CLI**:
```bash
python scripts/run_db_pipeline.py \
  --db data/ledger.db \
  --data-dir data \
  --accounts config/accounts.yml
```

**Behavior**:
1. Executes CSV → DB migration
2. Runs additional normalized description backfill pass
3. Exports Firefly CSVs back from DB view by bank (`data/<bank_id>/firefly_<bank_id>.csv`)

### Firefly Export Service

**File**: `src/services/firefly_export_service.py`

**Key Function**:

```python
export_firefly_csv_from_db(
    db_path: Path,
    out_csv: Path,
    bank_id: Optional[str] = None,
    use_normalized_description: bool = False,
) -> int
# Exports Firefly-compatible CSV from SQL view `firefly_export`
```

## Service Patterns

### Dependency Injection
Services receive paths/config as parameters, not hardcoded

### Error Handling
Return tuples: `(success: bool, result: Dict)` for operations

### Logging
All services use structured logging from `logging_config.py`

### Atomic Operations
File writes use temp file → rename pattern

## Testing

**Location**: `tests/test_*_service.py`

**Coverage**:
- Import service: 3 tests
- Rule service: 2 tests
- Analytics service: 24 tests
- Data service: 11 tests

## Usage Examples

### Import Workflow
```python
from services.import_service import resolve_output_paths, copy_csv_to_analysis

paths = resolve_output_paths("santander_likeu", target_mapping)
csv_path = paths["csv_output"]

# Write transactions to CSV...

# Copy to analysis directory
copy_csv_to_analysis(csv_path, "santander_likeu")
```

### Rule Correction Workflow
```python
from services.rule_service import stage_rule_change, merge_pending_rules

# Stage rule
ok, result = stage_rule_change(
    rules_path=Path("config/rules.yml"),
    pending_path=Path("config/rules.pending.yml"),
    merchant_name="OXXO",
    regex_pattern="OXXO.*",
    expense_account="Expenses:Food:Groceries",
    bucket_tag="groceries"
)

if ok:
    # Merge when ready
    ok, merge_result = merge_pending_rules(
        rules_path,
        pending_path,
        backup_dir=Path("config/backups")
    )
```

### Analytics Dashboard
```python
from services.data_service import load_transactions
from services.analytics_service import calculate_categorization_stats

df = load_transactions("santander_likeu", prefer_db=True)

# All time stats
stats = calculate_categorization_stats(df)

# Filtered by period
stats = calculate_categorization_stats(df, period="2024-01")

# Filtered by date range
stats = calculate_categorization_stats(
    df,
    start_date=datetime(2024, 1, 1),
    end_date=datetime(2024, 1, 31)
)
```

## CodeGraph Navigation for Services

Use CodeGraph to trace service dependencies before modifying business logic:

```bash
# Find all service functions
codegraph_search "analytics_service"
codegraph_search "import_service"
codegraph_search "rule_service"

# Trace who calls a service function (UI → service)
codegraph_callers "calculate_categorization_stats"
codegraph_callers "resolve_output_paths"

# Trace what a service function calls (service → domain)
codegraph_callees "merge_pending_rules"
codegraph_callees "load_transactions_from_csv"

# Check blast radius before changing service signatures
codegraph_impact "stage_rule_change"
codegraph_impact "copy_csv_to_analysis"

# Get full source for implementation context
codegraph_node "calculate_categorization_stats"
```

**Before modifying any service function:**
1. `codegraph_impact "<function>"` — find all callers (UI + tests)
2. `codegraph_callees "<function>"` — identify mocking targets for tests
3. Update function + update callers + update tests together

## Related Files

- `src/domain/transaction.py` - Domain models used by services
- `src/ui/pages/*.py` - UI calls services
- `config/rules.yml` - Configuration loaded by services
- `src/database/schema.sql` - SQLite schema
- `src/csv_to_db_migrator.py` - CSV migration orchestration
- `src/db_pipeline.py` - one-command migration+export pipeline
- `src/services/firefly_export_service.py` - DB view to Firefly CSV export
- `tests/test_*_service.py` - Service tests
